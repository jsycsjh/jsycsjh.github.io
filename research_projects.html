<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>

<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />

<title>Research </title>
</head>
<body>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="research_projects.html">Research</a></div>
<div class="menu-item"><a href="services.html">Services</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>

<p>Below, I describe the major themes of my research.</p>

<h2>Optimization and AutoML</h2>





<input
  type="checkbox"
  id="toggle_testSection"
  class="toggle_collapse"
  hidden
  aria-hidden="true"
>
<div>
  <p class="collapseAfter">
    We showed theoretically how hyperparameter configurations in Stochastic Gradient Descent Momentum connect to the generalization of optimizers, optimized the hyperparameters in a principled fashion, and improved over existing auto-tuners on image classification tasks.
  </p><p>
    We study the convergence and generalization of a large class of Stochastic Gradient Descent (SGD) momentum schemes, in both learning from scratch and transferring representations with fine-tuning. 
    Momentum-based acceleration of SGD is the default optimizer for many deep learning models. However, there is a lack of general convergence guarantees for many existing momentum variants in conjunction withstochastic gradient. 
    It is also unclear how the momentum methods may affect thegeneralization error. We give a unified analysis of several popular optimizers, e.g., Polyak's heavy ball momentum and Nesterov's accelerated gradient. 
    Our contribution is threefold. First, we give a unified convergence guarantee for a large class of momentum variants in thestochastic setting. Notably, our results cover both convex and nonconvex objectives. Second, we prove a generalization bound for neural networks trained by momentum variants. 
    We analyze how hyperparameters affect the generalization bound and consequently propose guidelines on how to tune these hyperparameters in various momentum schemes to generalize well. 
    We provide extensive empirical evidence to our proposed guidelines. Third, this study fills the vacancy of a formal analysis of fine-tuning in literature. To our best knowledge, our work is the first systematic generalizability analysis on momentum methods that cover both learning from scratch and fine-tuning.
    </p>
</div>
<label for="toggle_testSection" hidden aria-hidden="true"></label>

<h2>Adversarial Attacks on Deep Models</h2>

<p>We built an attacker that could jeopardize the interpretation of deep reinforcement learning, and showed theoretically why this attacker is difficult to defend against.</p>


<input
  type="checkbox"
  id="toggle_testSection_1"
  class="toggle_collapse"
  hidden
  aria-hidden="true"
>
<div>
  <p class="collapseAfter">
    We showed theoretically how hyperparameter configurations in Stochastic Gradient Descent Momentum connect to the generalization of optimizers, optimized the hyperparameters in a principled fashion, and improved over existing auto-tuners on image classification tasks.
  </p><p>
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus vel nibh et dolor dignissim tempor vel vel ante. Fusce vehicula vitae elit eu commodo. Vivamus consectetur diam vitae ligula gravida, sit amet dignissim libero ultricies. Sed at efficitur dolor, ut eleifend arcu. Ut porta volutpat lobortis. Nullam sit amet est ac mauris sollicitudin ultricies. Donec maximus, velit vel lacinia faucibus, odio urna suscipit sem, at tincidunt tortor leo id orci. Nam iaculis tristique ligula. Vivamus urna ligula, iaculis a tellus sed, feugiat venenatis tellus. Maecenas ullamcorper neque at vestibulum cursus. Ut urna sapien, tempus ut ornare eget, ultrices vitae nisl.
  <p><p>
    Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis commodo dui lectus, id elementum lectus condimentum a. Maecenas feugiat ornare lorem, ac tincidunt purus ultricies eget. Ut fringilla pharetra mi sed sodales. Cras accumsan eleifend massa, et ullamcorper ante tempus ut. Nam dolor diam, finibus a felis et, lacinia viverra dui. Donec magna leo, vestibulum id urna sit amet, dapibus pharetra orci. Sed sodales et erat sed egestas. Phasellus condimentum tempus urna, id facilisis ligula maximus vel.
  </p>
</div>
<label for="toggle_testSection_1" hidden aria-hidden="true"></label>

<h2>Correlation Network: A Lightweight Booster for Extreme Multi-label Text Classifier</h2>

<p>We proposed a correlation layer that could be easily added on top of any extreme multi-label text classifier while enhancing its prediction accuracy.</p>


<h2>Multi-directional Attention Learning to Impute Multivariate Time Series</h2>

<p>We proposed an attention-based framework that could effectively capture long-term dependencies between multivariate time series of varying missingness.</p>


<h2>Spatial Temporal Analysis of Multi-Subject, Stimulus-evoked fMRI Data Modeling</h2>

<p>We proposed a low rank multivariate General Linear Model for multi-subject stimulus-evoked fMRI data, implemented efficient optimization algorithm to estimate Hemodynamic Response Function (HRF) and to locate activated brain areas. (joint work with Department of Statistics & School of Medicine, University of Virginia)
</p>


<h2>Splicing of Multi-Scale &ldquo;Downscaler&rdquo; Air Quality Surfaces</h2>

<p>We worked on a methodological framework for splicing NOAA regional estimates of &ldquo;Downscaler&rdquo;, EPA's air quality monitoring data fusion product, to create a smooth national surface. (joint work with US EPA)</p>  


<h2>Mediation Process of Online Cognitive Bias Modification (CBM) Intervention for Patients with Negative Prospection</h2>

<p>We applied parallel process latent growth curve modeling to study the mediation effect of psychological intervention on negative prospection. (joint work with Department of Psychology, University of Virginia)</p>  


</td>
</tr>
</table>
</body>

</html>