<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>

<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />

<title>Research </title>
</head>
<body>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="research_projects.html">Research</a></div>
<div class="menu-item"><a href="services.html">Services</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>

<p>Below, I describe the major themes of my research.</p>

<h2>Optimization and AutoML</h2>





<input
  type="checkbox"
  id="toggle_testSection"
  class="toggle_collapse"
  hidden
  aria-hidden="true"
>
<div>
  <p class="collapseAfter">
    We showed theoretically how hyperparameter configurations in Stochastic Gradient Descent Momentum connect to the generalization of optimizers, optimized the hyperparameters in a principled fashion, and improved over existing auto-tuners on image classification tasks.
  </p><p>
    We study the convergence and generalization of a large class of Stochastic Gradient Descent (SGD) momentum schemes, in both learning from scratch and transferring representations with fine-tuning. 
    Momentum-based acceleration of SGD is the default optimizer for many deep learning models. However, there is a lack of general convergence guarantees for many existing momentum variants in conjunction withstochastic gradient. 
    It is also unclear how the momentum methods may affect thegeneralization error. We give a unified analysis of several popular optimizers, e.g., Polyak's heavy ball momentum and Nesterov's accelerated gradient. 
    Our contribution is threefold. First, we give a unified convergence guarantee for a large class of momentum variants in thestochastic setting. Notably, our results cover both convex and nonconvex objectives. Second, we prove a generalization bound for neural networks trained by momentum variants. 
    We analyze how hyperparameters affect the generalization bound and consequently propose guidelines on how to tune these hyperparameters in various momentum schemes to generalize well. 
    We provide extensive empirical evidence to our proposed guidelines. Third, this study fills the vacancy of a formal analysis of fine-tuning in literature. To our best knowledge, our work is the first systematic generalizability analysis on momentum methods that cover both learning from scratch and fine-tuning.
    </p>
</div>
<label for="toggle_testSection" hidden aria-hidden="true"></label>

<h2>Adversarial Attacks on Deep Models</h2>

<input
  type="checkbox"
  id="toggle_testSection_1"
  class="toggle_collapse"
  hidden
  aria-hidden="true"
>
<div>
  <p class="collapseAfter">
    We built an attacker that could jeopardize the interpretation of deep reinforcement learning, and showed theoretically why this attacker is difficult to defend against.
  </p><p>
    The past years have witnessed the rapid development of deep reinforcement learning (DRL), which is a combination of deep learning and reinforcement learning (RL). 
    However, the adoption of deep neural networks makes the decision-making process of DRL opaque and lacking transparency. Motivated by this, various interpretation methods for DRL have been proposed. 
    However, those interpretation methods make an implicit assumption that they are performed in a reliable and secure environment. In practice, sequential agent-environment interactions expose the DRL algorithms and their corresponding downstream interpretations to extra adversarial risk. 
    In spite of the prevalence of malicious attacks, there is no existing work studying the possibility and feasibility of malicious attacks against DRL interpretations. 
    To bridge this gap, we investigate the vulnerability of DRL interpretation methods. Specifically, we introduce the first study of the adversarial attacks against DRL interpretations, and propose an optimization framework based on which the optimal adversarial attack strategy can be derived. 
    In addition, we study the vulnerability of DRL interpretation methods to the model poisoning attacks, and present an algorithmic framework to rigorously formulate the proposed model poisoning attack. 
    Finally, we conduct both theoretical analysis and extensive experiments to validate the effectiveness of the proposed malicious attacks against DRL interpretations.</p>
</div>
<label for="toggle_testSection_1" hidden aria-hidden="true"></label>

<h2>Correlation Network: A Lightweight Booster for Extreme Multi-label Text Classifier</h2>

<p>We proposed a correlation layer that could be easily added on top of any extreme multi-label text classifier while enhancing its prediction accuracy.</p>


<h2>Multi-directional Attention Learning to Impute Multivariate Time Series</h2>

<p>We proposed an attention-based framework that could effectively capture long-term dependencies between multivariate time series of varying missingness.</p>


<h2>Spatial Temporal Analysis of Multi-Subject, Stimulus-evoked fMRI Data Modeling</h2>

<p>We proposed a low rank multivariate General Linear Model for multi-subject stimulus-evoked fMRI data, implemented efficient optimization algorithm to estimate Hemodynamic Response Function (HRF) and to locate activated brain areas. (joint work with Department of Statistics & School of Medicine, University of Virginia)
</p>


<h2>Splicing of Multi-Scale &ldquo;Downscaler&rdquo; Air Quality Surfaces</h2>

<p>We worked on a methodological framework for splicing NOAA regional estimates of &ldquo;Downscaler&rdquo;, EPA's air quality monitoring data fusion product, to create a smooth national surface. (joint work with US EPA)</p>  


<h2>Mediation Process of Online Cognitive Bias Modification (CBM) Intervention for Patients with Negative Prospection</h2>

<p>We applied parallel process latent growth curve modeling to study the mediation effect of psychological intervention on negative prospection. (joint work with Department of Psychology, University of Virginia)</p>  


</td>
</tr>
</table>
</body>

</html>